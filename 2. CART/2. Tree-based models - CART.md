# Tree-based models - CART

In this section we will discuss the use of CART (**C**lassification **A**nd **R**egression **T**rees) and more specifically, we will use regression trees to predict claim frequencies.


```R
require("arrow")
# require("CASdatasets") - Not necessary, as we saved the data as parquet file
require("rpart")
require("rpart.plot")
require("caret")
require("ggplot2")


options(repr.plot.width = 8, repr.plot.height = 6, repr.plot.res = 150)
```


```R
dataset = read_parquet(file = "../data/dataset.parquet")
```

When performing machine learning, one way to assess the predictability of the model is to use *fresh* data, not used in the *learning* process and compute some metric. This is also a good way to check whether the model is overfitting the data.

In order to have *fresh* data available at a later stage, it is common to split the dataset into two:
 - a training set. This data is used to estimate/learn the model.
 - a test set. This data is used to check the performance of the model, once it has been learned.
 
The package *caret* provides a lot of helpful functions. One of them is *createDataPartition*. Let us now split the data into two pieces (80% - 20%), by relying on stratified sampling. We use stratified sampling to obtain a similar distribution of the number of claims in boths the training and the test set. 


```R
set.seed(21)
in_training = createDataPartition(dataset$ClaimNb, times = 1, p = 0.8, list = FALSE)
training_set = dataset[in_training, ]
testing_set = dataset[-in_training, ]
```

Let us check the distribution of the variable *ClaimNb* among these two groups.


```R
temp = dataset
temp$training = 0
temp[in_training, "in_train_set"] = 1
temp[-in_training, "in_train_set"] = 0

summary_table = 100*round(prop.table(with(temp, table(in_train_set, ClaimNb)), 1), 5)
summary_table
rm(temp)
```


                ClaimNb
    in_train_set      0      1      2      3      4
               0 96.316  3.502  0.174  0.006  0.001
               1 96.254  3.562  0.176  0.007  0.001


## Package CART

We will use the package CART which allows to compute regression trees. rpart can be used for regression and classification. It also implements a method for *Poisson* data.

### Quick Example

Let us start with a simple example, using two covariates:




```R
m0_rpart = rpart(cbind(Exposure, ClaimNb) ~ DriverAge + CarAge, 
                 data = training_set,
                 method = "poisson", 
                 control = rpart.control(cp = 0.01))
summary(m0_rpart)
```

    Call:
    rpart(formula = cbind(Exposure, ClaimNb) ~ DriverAge + CarAge, 
        data = training_set, method = "poisson", control = rpart.control(cp = 0.01))
      n= 328692 
    
               CP nsplit rel error xerror xstd
    1 0.006462633      0         1      0    0
    
    Node number 1: 328692 observations
      events=12944,  estimated rate=0.07025564 , mean deviance=0.256468 
    
    

It appears that the tree has a single node and has not been split further. This comes from the complexity parameter which penalizes the splitting. **By default**, the complexity parameter **cp = 0.01**, which is often too large for Poisson data with low frequencies.

Let us put **cp = 0**, but to keep a small tree we will also impose a maximum depth of 3.


```R
m0_rpart = rpart(cbind(Exposure, ClaimNb) ~ DriverAge + CarAge, 
                 data = training_set,
                 control = rpart.control(cp = 0, maxdepth = 3))
summary(m0_rpart)
```

    Call:
    rpart(formula = cbind(Exposure, ClaimNb) ~ DriverAge + CarAge, 
        data = training_set, control = rpart.control(cp = 0, maxdepth = 3))
      n= 328692 
    
                CP nsplit rel error    xerror        xstd
    1 0.0064626327      0 1.0000000 1.0000165 0.006849924
    2 0.0013751350      1 0.9935374 0.9941993 0.006790180
    3 0.0011573505      2 0.9921622 0.9930955 0.006786100
    4 0.0006268552      3 0.9910049 0.9918054 0.006778356
    5 0.0003327416      4 0.9903780 0.9911298 0.006771821
    6 0.0003102204      5 0.9900453 0.9910272 0.006771428
    7 0.0001005746      6 0.9897351 0.9906720 0.006766914
    8 0.0000000000      7 0.9896345 0.9905635 0.006767021
    
    Variable importance
    DriverAge    CarAge 
           88        12 
    
    Node number 1: 328692 observations,    complexity param=0.006462633
      events=12944,  estimated rate=0.07025564 , mean deviance=0.256468 
      left son=2 (302869 obs) right son=3 (25823 obs)
      Primary splits:
          DriverAge < 26.5 to the right, improve=544.79390, (0 missing)
          CarAge    < 12.5 to the right, improve= 94.27821, (0 missing)
    
    Node number 2: 302869 observations,    complexity param=0.001157351
      events=11443,  estimated rate=0.06615313 , mean deviance=0.2471847 
      left son=4 (61175 obs) right son=5 (241694 obs)
      Primary splits:
          CarAge    < 12.5 to the right, improve=97.56350, (0 missing)
          DriverAge < 54.5 to the right, improve=76.38098, (0 missing)
    
    Node number 3: 25823 observations,    complexity param=0.001375135
      events=1501,  estimated rate=0.1331876 , mean deviance=0.3442522 
      left son=6 (22085 obs) right son=7 (3738 obs)
      Primary splits:
          DriverAge < 20.5 to the right, improve=115.93870, (0 missing)
          CarAge    < 4.5  to the left,  improve= 14.68241, (0 missing)
    
    Node number 4: 61175 observations,    complexity param=0.0003327416
      events=2066,  estimated rate=0.0548382 , mean deviance=0.2365854 
      left son=8 (15400 obs) right son=9 (45775 obs)
      Primary splits:
          DriverAge < 54.5 to the right, improve=28.05002, (0 missing)
          CarAge    < 17.5 to the right, improve=15.18412, (0 missing)
    
    Node number 5: 241694 observations,    complexity param=0.0006268552
      events=9377,  estimated rate=0.06930542 , mean deviance=0.2494638 
      left son=10 (66211 obs) right son=11 (175483 obs)
      Primary splits:
          DriverAge < 54.5 to the right, improve=52.843270, (0 missing)
          CarAge    < 10.5 to the right, improve= 1.787395, (0 missing)
    
    Node number 6: 22085 observations,    complexity param=0.0003102204
      events=1155,  estimated rate=0.1176422 , mean deviance=0.31826 
      left son=12 (13410 obs) right son=13 (8675 obs)
      Primary splits:
          DriverAge < 23.5 to the right, improve=26.15309, (0 missing)
          CarAge    < 13.5 to the right, improve=14.77670, (0 missing)
      Surrogate splits:
          CarAge < 22.5 to the left,  agree=0.608, adj=0.001, (0 split)
    
    Node number 7: 3738 observations,    complexity param=0.0001005746
      events=346,  estimated rate=0.236835 , mean deviance=0.4668084 
      left son=14 (822 obs) right son=15 (2916 obs)
      Primary splits:
          CarAge    < 14.5 to the right, improve=8.521172, (0 missing)
          DriverAge < 19.5 to the right, improve=6.852553, (0 missing)
    
    Node number 8: 15400 observations
      events=509,  estimated rate=0.04525469 , mean deviance=0.2331214 
    
    Node number 9: 45775 observations
      events=1557,  estimated rate=0.05893169 , mean deviance=0.237138 
    
    Node number 10: 66211 observations
      events=2567,  estimated rate=0.06159574 , mean deviance=0.2504668 
    
    Node number 11: 175483 observations
      events=6810,  estimated rate=0.07273823 , mean deviance=0.2487842 
    
    Node number 12: 13410 observations
      events=625,  estimated rate=0.103481 , mean deviance=0.2901859 
    
    Node number 13: 8675 observations
      events=530,  estimated rate=0.1400605 , mean deviance=0.358643 
    
    Node number 14: 822 observations
      events=50,  estimated rate=0.1634128 , mean deviance=0.3731759 
    
    Node number 15: 2916 observations
      events=296,  estimated rate=0.2544343 , mean deviance=0.4902952 
    
    

The easiest way to interpret a CART is probably to plot it (if it is not too large, though!). This can be achieved with the function *rpart.plot* from the package *rpart.plot*.


```R
rpart.plot(m0_rpart, 
           type = 5, 
           extra = 101, 
           under = FALSE, 
           fallen.leaves = TRUE,
           digits = 3)
```


    
![png](2.%20Tree-based%20models%20-%20CART_files/2.%20Tree-based%20models%20-%20CART_12_0.png)
    


If the tree is too large, we will probably have some overfitting. To prevent overfitting, we can play with the complexity parameter cp. A good approach is to compute the whole tree, without any penalty (i.e. complexity parameter is set to 0) and afterwards prune to tree.


```R
m0_rpart = rpart(cbind(Exposure, ClaimNb) ~ DriverAge + CarAge, 
                 data = training_set,
                 control = rpart.control(cp = 0))
rpart.plot(m0_rpart)
```

    Warning message:
    "labs do not fit even at cex 0.15, there may be some overplotting"
    


    
![png](2.%20Tree-based%20models%20-%20CART_files/2.%20Tree-based%20models%20-%20CART_14_1.png)
    



```R
rpart.plot(prune(m0_rpart, cp = 9e-04), 
           type = 5, 
           extra = 101, 
           under = FALSE, 
           fallen.leaves = TRUE,
           digits = 3)
```


    
![png](2.%20Tree-based%20models%20-%20CART_files/2.%20Tree-based%20models%20-%20CART_15_0.png)
    


We also see that in some terminal nodes (i.e. leaves), the number of observations (and of claims) is very low. We can set a minimum number of observation in any terminal node using minbucket


```R
m0_rpart = rpart(cbind(Exposure, ClaimNb) ~ DriverAge + CarAge, 
                 data = training_set,
                 control = rpart.control(cp = 0, maxdepth = 3, minbucket = 1000))
rpart.plot(m0_rpart)
```


    
![png](2.%20Tree-based%20models%20-%20CART_files/2.%20Tree-based%20models%20-%20CART_17_0.png)
    


## Complexity Parameter

Playing around with the complexity parameter will yield sub-trees of the fully developped tree (i.e. the one with cp = 0).

We check verify on one example that increasing the complexity parameter *cp* will only prune the tree differently and that by increasing *cp*, we will obtain a subtree. The common parts of the three trees below (i.e., the first split) are identical.


```R
tree1 = rpart(cbind(Exposure, ClaimNb) ~ DriverAge + CarAge, 
                 data = training_set,
                 control = rpart.control(cp = 0.0063))
rpart.plot(tree1,
           type = 5, 
           extra = 101, 
           under = FALSE, 
           fallen.leaves = TRUE,
           digits = 3)

tree2 = rpart(cbind(Exposure, ClaimNb) ~ DriverAge + CarAge, 
                 data = training_set,
                 control = rpart.control(cp = 0.0013))
rpart.plot(tree2,
           type = 5, 
           extra = 101, 
           under = FALSE, 
           fallen.leaves = TRUE,
           digits = 3)


tree3 = rpart(cbind(Exposure, ClaimNb) ~ DriverAge + CarAge, 
                 data = training_set,
                 control = rpart.control(cp = 0.0011))
rpart.plot(tree3,
           type = 5, 
           extra = 101, 
           under = FALSE, 
           fallen.leaves = TRUE,
           digits = 3)
```


    
![png](2.%20Tree-based%20models%20-%20CART_files/2.%20Tree-based%20models%20-%20CART_19_0.png)
    



    
![png](2.%20Tree-based%20models%20-%20CART_files/2.%20Tree-based%20models%20-%20CART_19_1.png)
    



    
![png](2.%20Tree-based%20models%20-%20CART_files/2.%20Tree-based%20models%20-%20CART_19_2.png)
    


## Cross-validation

Let us now find the optimal tree, by using cross-validation. We will again only use the variable DriverAge and CarAge in this section. By default, rpart will perform 10-fold cross-validation, using the option xval = 10. (Remark: The whole process of how the cross-validation is operated in described in Section 4.2 of rpart’s vignette: https://cran.r-project.org/web/packages/rpart/vignettes/longintro.pdf).

Essentially (and with some shortcuts), we can summarize the method as follows:

1. Estimate the full tree (cp = 0), on the whole data. We obtain a list of complexity parameters $\alpha_i$. We compute the $\beta$s are the geometric mean of two successive $\alpha_i$s. To each $\beta_i$ corresponds a subtree of the full tree.
2. Split the whole data into *folds* (generally 10 folds).
    - Estimate the tree using all folds but one. 
    - Using the list of $\beta_i$s from the previous step, we prune the tree to differents subtrees.
    - Compute the loss function ("risk") of these subtrees on the left-out fold. For each $\beta_i$ we obtain a loss.
    - Loop on the folds. We obtain for each $\beta_i$ as many losses as there are folds. We aggregate these losses by summing.
    - After agregation, for each $\beta_i$ we have a single loss.
3. The optimal complexity parameter $cp^*$ is defined as the $\beta_i$ with the smallest loss (see remark below). The optimal tree is defined as the subtree with cp = $cp^*$

Remark: We may also use the 1-SE rule, which will yield the largest $\beta_i$ whose corresponding loss is in the 1-standard error window of the smallest loss.


```R
m0_rpart = rpart(cbind(Exposure, ClaimNb) ~ DriverAge + CarAge, 
                 data = training_set,
                 control = rpart.control(
                             cp = 3e-5, 
                             xval = 10))
printcp(m0_rpart)
```

    
    Rates regression tree:
    rpart(formula = cbind(Exposure, ClaimNb) ~ DriverAge + CarAge, 
        data = training_set, control = rpart.control(cp = 3e-05, 
            xval = 10))
    
    Variables actually used in tree construction:
    [1] CarAge    DriverAge
    
    Root node error: 84299/328692 = 0.25647
    
    n= 328692 
    
               CP nsplit rel error  xerror      xstd
    1  6.4626e-03      0   1.00000 1.00002 0.0068500
    2  1.3751e-03      1   0.99354 0.99406 0.0067905
    3  1.1574e-03      2   0.99216 0.99328 0.0067881
    4  6.2686e-04      3   0.99100 0.99200 0.0067801
    5  3.3274e-04      4   0.99038 0.99066 0.0067666
    6  3.1022e-04      5   0.99005 0.99058 0.0067669
    7  2.1284e-04      6   0.98974 0.99022 0.0067626
    8  1.4038e-04      7   0.98952 0.99021 0.0067629
    9  1.2139e-04      8   0.98938 0.99043 0.0067662
    10 1.2074e-04      9   0.98926 0.99044 0.0067657
    11 1.0057e-04     11   0.98902 0.99053 0.0067667
    12 9.6270e-05     12   0.98892 0.99054 0.0067664
    13 8.4053e-05     15   0.98863 0.99061 0.0067680
    14 8.1808e-05     16   0.98855 0.99061 0.0067685
    15 7.8559e-05     17   0.98846 0.99065 0.0067686
    16 7.0412e-05     19   0.98831 0.99075 0.0067705
    17 6.6283e-05     20   0.98824 0.99073 0.0067708
    18 6.1079e-05     24   0.98797 0.99082 0.0067723
    19 5.8268e-05     27   0.98779 0.99137 0.0067786
    20 5.4058e-05     28   0.98773 0.99199 0.0067860
    21 5.1288e-05     29   0.98768 0.99219 0.0067874
    22 5.0273e-05     30   0.98762 0.99251 0.0067903
    23 4.9925e-05     31   0.98757 0.99262 0.0067916
    24 4.8073e-05     33   0.98747 0.99265 0.0067913
    25 4.7714e-05     35   0.98738 0.99275 0.0067933
    26 4.6502e-05     36   0.98733 0.99292 0.0067946
    27 4.6344e-05     40   0.98713 0.99299 0.0067958
    28 4.6192e-05     42   0.98703 0.99303 0.0067961
    29 4.5242e-05     48   0.98674 0.99309 0.0067964
    30 4.4525e-05     53   0.98652 0.99312 0.0067966
    31 4.4492e-05     55   0.98643 0.99323 0.0067978
    32 4.3932e-05     57   0.98634 0.99329 0.0067984
    33 4.3542e-05     59   0.98625 0.99343 0.0068008
    34 4.2898e-05     60   0.98621 0.99357 0.0068031
    35 4.2262e-05     61   0.98616 0.99370 0.0068050
    36 4.0081e-05     62   0.98612 0.99406 0.0068085
    37 4.0055e-05     63   0.98608 0.99438 0.0068128
    38 3.9998e-05     65   0.98600 0.99438 0.0068130
    39 3.9919e-05     66   0.98596 0.99448 0.0068137
    40 3.9307e-05     67   0.98592 0.99448 0.0068138
    41 3.8898e-05     72   0.98570 0.99470 0.0068167
    42 3.8644e-05     74   0.98563 0.99467 0.0068159
    43 3.8431e-05     80   0.98540 0.99477 0.0068170
    44 3.7872e-05     81   0.98536 0.99490 0.0068192
    45 3.7232e-05     89   0.98504 0.99515 0.0068241
    46 3.6437e-05     90   0.98501 0.99543 0.0068288
    47 3.6379e-05     93   0.98490 0.99576 0.0068325
    48 3.5700e-05     94   0.98486 0.99583 0.0068341
    49 3.5516e-05     97   0.98475 0.99595 0.0068353
    50 3.5412e-05     99   0.98468 0.99600 0.0068366
    51 3.5039e-05    103   0.98454 0.99606 0.0068375
    52 3.4827e-05    104   0.98451 0.99608 0.0068396
    53 3.4762e-05    109   0.98433 0.99616 0.0068412
    54 3.4528e-05    117   0.98405 0.99616 0.0068413
    55 3.3898e-05    126   0.98371 0.99636 0.0068433
    56 3.3539e-05    130   0.98357 0.99653 0.0068450
    57 3.3366e-05    131   0.98354 0.99661 0.0068477
    58 3.3340e-05    133   0.98347 0.99666 0.0068480
    59 3.2688e-05    134   0.98344 0.99687 0.0068514
    60 3.2282e-05    137   0.98334 0.99716 0.0068545
    61 3.2223e-05    141   0.98320 0.99731 0.0068574
    62 3.2012e-05    144   0.98311 0.99738 0.0068584
    63 3.1876e-05    145   0.98308 0.99748 0.0068600
    64 3.1862e-05    147   0.98301 0.99756 0.0068606
    65 3.1779e-05    149   0.98295 0.99764 0.0068612
    66 3.1665e-05    150   0.98292 0.99777 0.0068623
    67 3.1632e-05    151   0.98288 0.99777 0.0068624
    68 3.1239e-05    152   0.98285 0.99776 0.0068625
    69 3.0696e-05    157   0.98269 0.99794 0.0068651
    70 3.0644e-05    161   0.98257 0.99792 0.0068646
    71 3.0455e-05    165   0.98245 0.99808 0.0068660
    72 3.0231e-05    167   0.98239 0.99810 0.0068663
    73 3.0230e-05    168   0.98236 0.99808 0.0068661
    74 3.0207e-05    175   0.98214 0.99808 0.0068661
    75 3.0110e-05    179   0.98201 0.99809 0.0068663
    76 3.0072e-05    181   0.98195 0.99810 0.0068665
    77 3.0020e-05    189   0.98171 0.99813 0.0068666
    78 3.0000e-05    191   0.98165 0.99821 0.0068672
    


```R
plotcp(m0_rpart)
```


    
![png](2.%20Tree-based%20models%20-%20CART_files/2.%20Tree-based%20models%20-%20CART_22_0.png)
    



```R
head(m0_rpart$cptable, 10)
```


<table class="dataframe">
<caption>A matrix: 10 × 5 of type dbl</caption>
<thead>
	<tr><th></th><th scope=col>CP</th><th scope=col>nsplit</th><th scope=col>rel error</th><th scope=col>xerror</th><th scope=col>xstd</th></tr>
</thead>
<tbody>
	<tr><th scope=row>1</th><td>0.0064626327</td><td>0</td><td>1.0000000</td><td>1.0000168</td><td>0.006849957</td></tr>
	<tr><th scope=row>2</th><td>0.0013751350</td><td>1</td><td>0.9935374</td><td>0.9940596</td><td>0.006790466</td></tr>
	<tr><th scope=row>3</th><td>0.0011573505</td><td>2</td><td>0.9921622</td><td>0.9932782</td><td>0.006788058</td></tr>
	<tr><th scope=row>4</th><td>0.0006268552</td><td>3</td><td>0.9910049</td><td>0.9920008</td><td>0.006780117</td></tr>
	<tr><th scope=row>5</th><td>0.0003327416</td><td>4</td><td>0.9903780</td><td>0.9906550</td><td>0.006766619</td></tr>
	<tr><th scope=row>6</th><td>0.0003102204</td><td>5</td><td>0.9900453</td><td>0.9905832</td><td>0.006766865</td></tr>
	<tr><th scope=row>7</th><td>0.0002128410</td><td>6</td><td>0.9897351</td><td>0.9902226</td><td>0.006762557</td></tr>
	<tr><th scope=row>8</th><td>0.0001403828</td><td>7</td><td>0.9895222</td><td>0.9902105</td><td>0.006762889</td></tr>
	<tr><th scope=row>9</th><td>0.0001213919</td><td>8</td><td>0.9893818</td><td>0.9904293</td><td>0.006766246</td></tr>
	<tr><th scope=row>10</th><td>0.0001207448</td><td>9</td><td>0.9892604</td><td>0.9904439</td><td>0.006765741</td></tr>
</tbody>
</table>



Let us see the optimal tree.


```R
cp_star = m0_rpart$cptable[which.min(m0_rpart$cptable[, 4]), 1]
print(cp_star)

rpart.plot(prune(m0_rpart, cp = cp_star), type = 5, extra = 101, under = FALSE, fallen.leaves = FALSE,
    digits = 3)
```

    [1] 0.0001403828
    


    
![png](2.%20Tree-based%20models%20-%20CART_files/2.%20Tree-based%20models%20-%20CART_25_1.png)
    


Using the 1-SE error, we need to compute the greatest cp parameters that enters the 1-SE window


```R
SE = m0_rpart$cptable[which.min(m0_rpart$cptable[, 4]), 5]
min_error = m0_rpart$cptable[which.min(m0_rpart$cptable[, 4]), 4]
boundary = min_error + SE
boundary

# We need to find the first cp such that xerror < boundary.
row_optimal = which(m0_rpart$cptable[, 4] < boundary)[1]
cp_star = m0_rpart$cptable[row_optimal, 1]
print(cp_star)

rpart.plot(prune(m0_rpart, cp = cp_star), type = 5, extra = 101, under = FALSE, fallen.leaves = FALSE,
    digits = 3)
```


0.996973385994474


    [1] 0.001375135
    


    
![png](2.%20Tree-based%20models%20-%20CART_files/2.%20Tree-based%20models%20-%20CART_27_2.png)
    


## Using all covariates

Let us now include all the covariates from the dataset


```R
m1_rpart = rpart(cbind(Exposure, ClaimNb) ~ Power + CarAge + DriverAge + Brand +
    Gas + Region + Density, data = training_set, method = "poisson", control = rpart.control(cp = 0,
    xval = 10, minbucket = 1000))
printcp(m1_rpart)
```

    
    Rates regression tree:
    rpart(formula = cbind(Exposure, ClaimNb) ~ Power + CarAge + DriverAge + 
        Brand + Gas + Region + Density, data = training_set, method = "poisson", 
        control = rpart.control(cp = 0, xval = 10, minbucket = 1000))
    
    Variables actually used in tree construction:
    [1] Brand     CarAge    Density   DriverAge Gas       Power     Region   
    
    Root node error: 84299/328692 = 0.25647
    
    n= 328692 
    
                CP nsplit rel error  xerror      xstd
    1   6.4626e-03      0   1.00000 1.00002 0.0068500
    2   4.3514e-03      1   0.99354 0.99406 0.0067904
    3   1.3751e-03      2   0.98919 0.98986 0.0067505
    4   8.3749e-04      3   0.98781 0.98877 0.0067440
    5   6.8456e-04      4   0.98697 0.98791 0.0067433
    6   5.2298e-04      5   0.98629 0.98723 0.0067401
    7   4.7610e-04      6   0.98577 0.98712 0.0067374
    8   4.4572e-04      7   0.98529 0.98710 0.0067368
    9   3.8233e-04      8   0.98484 0.98677 0.0067331
    10  2.9440e-04      9   0.98446 0.98664 0.0067347
    11  2.8062e-04     10   0.98417 0.98659 0.0067359
    12  2.7166e-04     11   0.98389 0.98638 0.0067344
    13  2.1860e-04     12   0.98361 0.98624 0.0067344
    14  2.0751e-04     13   0.98340 0.98608 0.0067338
    15  1.9694e-04     14   0.98319 0.98620 0.0067367
    16  1.8989e-04     15   0.98299 0.98625 0.0067397
    17  1.8985e-04     17   0.98261 0.98624 0.0067394
    18  1.8601e-04     18   0.98242 0.98624 0.0067394
    19  1.7769e-04     19   0.98224 0.98633 0.0067417
    20  1.6268e-04     20   0.98206 0.98652 0.0067434
    21  1.6045e-04     21   0.98190 0.98685 0.0067497
    22  1.5352e-04     22   0.98174 0.98674 0.0067488
    23  1.4807e-04     24   0.98143 0.98680 0.0067498
    24  1.4659e-04     25   0.98128 0.98706 0.0067540
    25  1.4407e-04     26   0.98113 0.98703 0.0067537
    26  1.4228e-04     27   0.98099 0.98723 0.0067563
    27  1.4202e-04     30   0.98056 0.98729 0.0067573
    28  1.3842e-04     31   0.98042 0.98745 0.0067604
    29  1.3315e-04     32   0.98028 0.98750 0.0067613
    30  1.2456e-04     33   0.98015 0.98759 0.0067629
    31  1.2273e-04     34   0.98002 0.98792 0.0067695
    32  1.2066e-04     35   0.97990 0.98789 0.0067689
    33  1.1524e-04     36   0.97978 0.98805 0.0067729
    34  1.1474e-04     37   0.97967 0.98815 0.0067735
    35  1.1243e-04     40   0.97932 0.98817 0.0067736
    36  1.1229e-04     42   0.97909 0.98830 0.0067749
    37  1.1198e-04     43   0.97898 0.98830 0.0067749
    38  1.0714e-04     47   0.97853 0.98833 0.0067758
    39  1.0319e-04     49   0.97832 0.98882 0.0067827
    40  1.0257e-04     51   0.97811 0.98891 0.0067835
    41  1.0219e-04     52   0.97801 0.98893 0.0067838
    42  1.0154e-04     56   0.97759 0.98892 0.0067840
    43  1.0035e-04     57   0.97749 0.98894 0.0067849
    44  9.9123e-05     58   0.97739 0.98902 0.0067855
    45  9.8518e-05     60   0.97719 0.98909 0.0067867
    46  9.8385e-05     63   0.97689 0.98912 0.0067872
    47  9.5185e-05     64   0.97679 0.98909 0.0067874
    48  9.4716e-05     65   0.97670 0.98942 0.0067915
    49  9.3310e-05     66   0.97660 0.98952 0.0067932
    50  9.2161e-05     67   0.97651 0.98955 0.0067939
    51  9.2131e-05     68   0.97642 0.98966 0.0067949
    52  9.0483e-05     69   0.97633 0.98968 0.0067951
    53  8.8080e-05     70   0.97623 0.98967 0.0067955
    54  8.7574e-05     73   0.97597 0.98985 0.0067994
    55  8.7510e-05     74   0.97588 0.98988 0.0067998
    56  8.4752e-05     76   0.97571 0.98996 0.0068006
    57  8.2477e-05     77   0.97562 0.99006 0.0068017
    58  7.9693e-05     78   0.97554 0.99020 0.0068019
    59  7.9347e-05     81   0.97530 0.99031 0.0068038
    60  7.8490e-05     85   0.97498 0.99033 0.0068039
    61  7.5804e-05     86   0.97491 0.99037 0.0068050
    62  7.3352e-05     88   0.97475 0.99045 0.0068066
    63  7.2257e-05     89   0.97468 0.99064 0.0068096
    64  7.1960e-05     90   0.97461 0.99064 0.0068095
    65  7.1906e-05     93   0.97439 0.99065 0.0068095
    66  7.1651e-05     94   0.97432 0.99072 0.0068106
    67  7.1248e-05     96   0.97418 0.99073 0.0068108
    68  7.1161e-05     97   0.97411 0.99078 0.0068112
    69  6.9819e-05     98   0.97404 0.99085 0.0068123
    70  6.9739e-05     99   0.97397 0.99109 0.0068149
    71  6.8798e-05    100   0.97390 0.99106 0.0068149
    72  6.7536e-05    101   0.97383 0.99112 0.0068155
    73  6.6512e-05    102   0.97376 0.99113 0.0068156
    74  6.6462e-05    103   0.97369 0.99120 0.0068168
    75  6.3150e-05    104   0.97363 0.99116 0.0068168
    76  6.2602e-05    106   0.97350 0.99122 0.0068187
    77  6.2347e-05    107   0.97344 0.99118 0.0068183
    78  6.1860e-05    108   0.97338 0.99120 0.0068186
    79  6.1721e-05    111   0.97319 0.99118 0.0068165
    80  6.1719e-05    112   0.97313 0.99118 0.0068165
    81  6.1004e-05    114   0.97300 0.99119 0.0068168
    82  6.0959e-05    117   0.97282 0.99115 0.0068165
    83  6.0768e-05    119   0.97270 0.99117 0.0068169
    84  6.0606e-05    121   0.97258 0.99119 0.0068169
    85  5.9010e-05    124   0.97240 0.99120 0.0068166
    86  5.8816e-05    126   0.97228 0.99124 0.0068186
    87  5.6879e-05    127   0.97222 0.99128 0.0068194
    88  5.6175e-05    129   0.97211 0.99138 0.0068201
    89  5.4762e-05    131   0.97199 0.99154 0.0068225
    90  5.4274e-05    133   0.97188 0.99160 0.0068232
    91  5.3731e-05    135   0.97177 0.99159 0.0068231
    92  5.2445e-05    139   0.97156 0.99168 0.0068252
    93  5.1946e-05    140   0.97151 0.99171 0.0068255
    94  5.1429e-05    141   0.97146 0.99171 0.0068253
    95  5.0994e-05    142   0.97140 0.99181 0.0068262
    96  4.7702e-05    143   0.97135 0.99177 0.0068266
    97  4.7492e-05    144   0.97131 0.99194 0.0068282
    98  4.7079e-05    145   0.97126 0.99194 0.0068287
    99  4.4908e-05    146   0.97121 0.99206 0.0068301
    100 4.4137e-05    147   0.97117 0.99214 0.0068313
    101 4.1810e-05    148   0.97112 0.99227 0.0068338
    102 4.1082e-05    149   0.97108 0.99240 0.0068355
    103 4.0846e-05    150   0.97104 0.99230 0.0068349
    104 4.0451e-05    151   0.97100 0.99230 0.0068349
    105 4.0295e-05    152   0.97096 0.99226 0.0068345
    106 3.7460e-05    155   0.97084 0.99233 0.0068355
    107 3.7366e-05    157   0.97076 0.99233 0.0068359
    108 3.7302e-05    158   0.97072 0.99237 0.0068364
    109 3.7234e-05    159   0.97069 0.99237 0.0068364
    110 3.5867e-05    161   0.97061 0.99247 0.0068373
    111 3.5196e-05    163   0.97054 0.99251 0.0068380
    112 3.4799e-05    164   0.97051 0.99255 0.0068383
    113 3.3286e-05    165   0.97047 0.99257 0.0068384
    114 3.2114e-05    166   0.97044 0.99259 0.0068383
    115 2.9743e-05    167   0.97041 0.99266 0.0068391
    116 2.9719e-05    169   0.97035 0.99270 0.0068397
    117 2.9135e-05    170   0.97032 0.99270 0.0068399
    118 2.6928e-05    171   0.97029 0.99271 0.0068399
    119 2.6240e-05    173   0.97023 0.99273 0.0068402
    120 2.4691e-05    174   0.97021 0.99273 0.0068400
    121 2.0154e-05    175   0.97018 0.99278 0.0068405
    122 1.9880e-05    176   0.97016 0.99274 0.0068402
    123 1.9549e-05    177   0.97014 0.99274 0.0068402
    124 1.8139e-05    178   0.97012 0.99273 0.0068402
    125 1.7866e-05    179   0.97010 0.99275 0.0068403
    126 1.7171e-05    180   0.97009 0.99275 0.0068404
    127 1.3957e-05    181   0.97007 0.99273 0.0068401
    128 1.3164e-05    182   0.97006 0.99270 0.0068399
    129 1.0606e-05    183   0.97004 0.99270 0.0068399
    130 5.0082e-06    184   0.97003 0.99271 0.0068400
    131 0.0000e+00    185   0.97003 0.99271 0.0068400
    


```R
plotcp(x = m1_rpart, minline = TRUE, col = "red")
```


    
![png](2.%20Tree-based%20models%20-%20CART_files/2.%20Tree-based%20models%20-%20CART_30_0.png)
    



```R
# Compute boundary for 1SE-rule
SE = m1_rpart$cptable[which.min(m1_rpart$cptable[, 4]), 5]
min_error = m1_rpart$cptable[which.min(m1_rpart$cptable[, 4]), 4]
boundary = min_error + SE

# Plot cross-validation error and 1SE_rule boundary.
ggplot() + geom_line(aes(x = m1_rpart$cptable[, 1], y = m1_rpart$cptable[, 4]))+
geom_point(aes(x = m1_rpart$cptable[, 1], y = m1_rpart$cptable[, 4]))+
geom_hline(aes(yintercept = boundary), color="red")+
scale_x_continuous(name = "CP") + scale_y_continuous(name = "xerror") + theme_bw()
```


    
![png](2.%20Tree-based%20models%20-%20CART_files/2.%20Tree-based%20models%20-%20CART_31_0.png)
    


If we take the value of cp that minimizes the error, we find


```R
cp_star = m1_rpart$cptable[which.min(m1_rpart$cptable[, 4]), 1]
cp_star
```


0.000207506742530253


Let us plot the optimal tree


```R
m2_rpart = prune(m1_rpart, cp = cp_star)
rpart.plot(m2_rpart, type = 5, extra = 101, under = FALSE, fallen.leaves = TRUE,
    digits = 3, cex = 0.60)
```


    
![png](2.%20Tree-based%20models%20-%20CART_files/2.%20Tree-based%20models%20-%20CART_35_0.png)
    


There is a possibility to extract a variable importance metric.


```R
plotdata = data.frame(m2_rpart$variable.importance)
names(plotdata) = 'importance'
plotdata$var = rownames(plotdata)

ggplot(plotdata,aes(x =reorder(var,importance), y=importance)) + 
geom_bar(stat='identity')+coord_flip()+
scale_x_discrete(name="Variable") + 
theme_bw()
```


    
![png](2.%20Tree-based%20models%20-%20CART_files/2.%20Tree-based%20models%20-%20CART_37_0.png)
    


Finally, let us compute the deviance on the testing_set.


```R
deviance_poisson = function(x_obs, x_pred) {
    2 * (sum(dpois(x = x_obs, lambda = x_obs, log = TRUE)) - sum(dpois(x = x_obs,
        lambda = x_pred, log = TRUE)))
}

deviance_poisson(x_obs = testing_set$ClaimNb, x_pred = predict(m2_rpart, testing_set) *
    testing_set$Exposure)
```


20514.3528453745


If we compute the deviance on the full tree (not the pruned tree), we obtain


```R
deviance_poisson(x_obs = testing_set$ClaimNb, x_pred = predict(m1_rpart, testing_set) *
    testing_set$Exposure)
```


20661.81536694


## Bagging of trees

Let us create the bootstrap samples.


```R
set.seed(85)
bootstrap_samples = createResample(training_set$ClaimNb, times = 50)
```

For each sample, we estimate a CART with the optimal complexity parameter found previously. Each tree, gives us an estimation of the claim frequency, which we average.


```R
bagg_cart = lapply(bootstrap_samples, function(X) {
    rpart(cbind(Exposure, ClaimNb) ~ Power + CarAge + DriverAge + Brand + Gas + Region + Density, 
          data = training_set[X, ], 
          method = "poisson", 
          control = rpart.control(cp = cp_star,
                                  xval = 0))
})
```


```R
pred = lapply(bagg_cart, function(X) {
    predict(X, testing_set) * testing_set$Exposure
})

pred = do.call(cbind, pred)
pred = apply(pred, 1, mean)
```


```R
deviance_poisson(x_obs = testing_set$ClaimNb, x_pred = pred)
```


20451.158368307

