{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92d7cef1",
   "metadata": {},
   "source": [
    "# Tree-based models - CART\n",
    "\n",
    "In this section we will discuss the use of CART (**C**lassification **A**nd **R**egression **T**rees) and more specifically, we will use regression trees to predict claim frequencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24f3081e",
   "metadata": {},
   "outputs": [],
   "source": [
    "require(\"arrow\")\n",
    "# require(\"CASdatasets\") - Not necessary, as we saved the data as parquet file\n",
    "require(\"rpart\")\n",
    "require(\"rpart.plot\")\n",
    "require(\"caret\")\n",
    "require(\"ggplot2\")\n",
    "\n",
    "\n",
    "options(repr.plot.width = 8, repr.plot.height = 6, repr.plot.res = 150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7120da5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = read_parquet(file = \"../data/dataset.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc137b8c",
   "metadata": {},
   "source": [
    "When performing machine learning, one way to assess the predictability of the model is to use *fresh* data, not used in the *learning* process and compute some metric. This is also a good way to check whether the model is overfitting the data.\n",
    "\n",
    "In order to have *fresh* data available at a later stage, it is common to split the dataset into two:\n",
    " - a training set. This data is used to estimate/learn the model.\n",
    " - a test set. This data is used to check the performance of the model, once it has been learned.\n",
    " \n",
    "The package *caret* provides a lot of helpful functions. One of them is *createDataPartition*. Let us now split the data into two pieces (80% - 20%), by relying on stratified sampling. We use stratified sampling to obtain a similar distribution of the number of claims in boths the training and the test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2558be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "set.seed(21)\n",
    "in_training = createDataPartition(dataset$ClaimNb, times = 1, p = 0.8, list = FALSE)\n",
    "training_set = dataset[in_training, ]\n",
    "testing_set = dataset[-in_training, ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4834b8fe",
   "metadata": {},
   "source": [
    "Let us check the distribution of the variable *ClaimNb* among these two groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb21324",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = dataset\n",
    "temp$training = 0\n",
    "temp[in_training, \"in_train_set\"] = 1\n",
    "temp[-in_training, \"in_train_set\"] = 0\n",
    "\n",
    "summary_table = 100*round(prop.table(with(temp, table(in_train_set, ClaimNb)), 1), 5)\n",
    "summary_table\n",
    "rm(temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dde55e7a",
   "metadata": {},
   "source": [
    "## Package CART\n",
    "\n",
    "We will use the package CART which allows to compute regression trees. rpart can be used for regression and classification. It also implements a method for *Poisson* data.\n",
    "\n",
    "### Quick Example\n",
    "\n",
    "Let us start with a simple example, using two covariates:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d85c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "m0_rpart = rpart(cbind(Exposure, ClaimNb) ~ DriverAge + CarAge, \n",
    "                 data = training_set,\n",
    "                 method = \"poisson\", \n",
    "                 control = rpart.control(cp = 0.01))\n",
    "summary(m0_rpart)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f4a134a",
   "metadata": {},
   "source": [
    "It appears that the tree has a single node and has not been split further. This comes from the complexity parameter which penalizes the splitting. **By default**, the complexity parameter **cp = 0.01**, which is often too large for Poisson data with low frequencies.\n",
    "\n",
    "Let us put **cp = 0**, but to keep a small tree we will also impose a maximum depth of 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f6439f",
   "metadata": {},
   "outputs": [],
   "source": [
    "m0_rpart = rpart(cbind(Exposure, ClaimNb) ~ DriverAge + CarAge, \n",
    "                 data = training_set,\n",
    "                 control = rpart.control(cp = 0, maxdepth = 3))\n",
    "summary(m0_rpart)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ae0c423",
   "metadata": {},
   "source": [
    "The easiest way to interpret a CART is probably to plot it (if it is not too large, though!). This can be achieved with the function *rpart.plot* from the package *rpart.plot*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e1f5792",
   "metadata": {},
   "outputs": [],
   "source": [
    "rpart.plot(m0_rpart, \n",
    "           type = 5, \n",
    "           extra = 101, \n",
    "           under = FALSE, \n",
    "           fallen.leaves = TRUE,\n",
    "           digits = 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6105ebb3",
   "metadata": {},
   "source": [
    "If the tree is too large, we will probably have some overfitting. To prevent overfitting, we can play with the complexity parameter cp. A good approach is to compute the whole tree, without any penalty (i.e. complexity parameter is set to 0) and afterwards prune to tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a560075a",
   "metadata": {},
   "outputs": [],
   "source": [
    "m0_rpart = rpart(cbind(Exposure, ClaimNb) ~ DriverAge + CarAge, \n",
    "                 data = training_set,\n",
    "                 control = rpart.control(cp = 0))\n",
    "rpart.plot(m0_rpart)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ef1e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "rpart.plot(prune(m0_rpart, cp = 9e-04), \n",
    "           type = 5, \n",
    "           extra = 101, \n",
    "           under = FALSE, \n",
    "           fallen.leaves = TRUE,\n",
    "           digits = 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "807ff72c",
   "metadata": {},
   "source": [
    "We also see that in some terminal nodes (i.e. leaves), the number of observations (and of claims) is very low. We can set a minimum number of observation in any terminal node using minbucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be0291ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "m0_rpart = rpart(cbind(Exposure, ClaimNb) ~ DriverAge + CarAge, \n",
    "                 data = training_set,\n",
    "                 control = rpart.control(cp = 0, maxdepth = 3, minbucket = 1000))\n",
    "rpart.plot(m0_rpart)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "159c9ac3",
   "metadata": {},
   "source": [
    "## Complexity Parameter\n",
    "\n",
    "Playing around with the complexity parameter will yield sub-trees of the fully developped tree (i.e. the one with cp = 0).\n",
    "\n",
    "We check verify on one example that increasing the complexity parameter *cp* will only prune the tree differently and that by increasing *cp*, we will obtain a subtree. The common parts of the three trees below (i.e., the first split) are identical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd926c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree1 = rpart(cbind(Exposure, ClaimNb) ~ DriverAge + CarAge, \n",
    "                 data = training_set,\n",
    "                 control = rpart.control(cp = 0.0063))\n",
    "rpart.plot(tree1,\n",
    "           type = 5, \n",
    "           extra = 101, \n",
    "           under = FALSE, \n",
    "           fallen.leaves = TRUE,\n",
    "           digits = 3)\n",
    "\n",
    "tree2 = rpart(cbind(Exposure, ClaimNb) ~ DriverAge + CarAge, \n",
    "                 data = training_set,\n",
    "                 control = rpart.control(cp = 0.0013))\n",
    "rpart.plot(tree2,\n",
    "           type = 5, \n",
    "           extra = 101, \n",
    "           under = FALSE, \n",
    "           fallen.leaves = TRUE,\n",
    "           digits = 3)\n",
    "\n",
    "\n",
    "tree3 = rpart(cbind(Exposure, ClaimNb) ~ DriverAge + CarAge, \n",
    "                 data = training_set,\n",
    "                 control = rpart.control(cp = 0.0011))\n",
    "rpart.plot(tree3,\n",
    "           type = 5, \n",
    "           extra = 101, \n",
    "           under = FALSE, \n",
    "           fallen.leaves = TRUE,\n",
    "           digits = 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "943a7e61",
   "metadata": {},
   "source": [
    "## Cross-validation\n",
    "\n",
    "Let us now find the optimal tree, by using cross-validation. We will again only use the variable DriverAge and CarAge in this section. By default, rpart will perform 10-fold cross-validation, using the option xval = 10. (Remark: The whole process of how the cross-validation is operated in described in Section 4.2 of rpartâ€™s vignette: https://cran.r-project.org/web/packages/rpart/vignettes/longintro.pdf).\n",
    "\n",
    "Essentially (and with some shortcuts), we can summarize the method as follows:\n",
    "\n",
    "1. Estimate the full tree (cp = 0), on the whole data. We obtain a list of complexity parameters $\\alpha_i$. We compute the $\\beta$s are the geometric mean of two successive $\\alpha_i$s. To each $\\beta_i$ corresponds a subtree of the full tree.\n",
    "2. Split the whole data into *folds* (generally 10 folds).\n",
    "    - Estimate the tree using all folds but one. \n",
    "    - Using the list of $\\beta_i$s from the previous step, we prune the tree to differents subtrees.\n",
    "    - Compute the loss function (\"risk\") of these subtrees on the left-out fold. For each $\\beta_i$ we obtain a loss.\n",
    "    - Loop on the folds. We obtain for each $\\beta_i$ as many losses as there are folds. We aggregate these losses by summing.\n",
    "    - After agregation, for each $\\beta_i$ we have a single loss.\n",
    "3. The optimal complexity parameter $cp^*$ is defined as the $\\beta_i$ with the smallest loss (see remark below). The optimal tree is defined as the subtree with cp = $cp^*$\n",
    "\n",
    "Remark: We may also use the 1-SE rule, which will yield the largest $\\beta_i$ whose corresponding loss is in the 1-standard error window of the smallest loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a04bf69f",
   "metadata": {},
   "outputs": [],
   "source": [
    "m0_rpart = rpart(cbind(Exposure, ClaimNb) ~ DriverAge + CarAge, \n",
    "                 data = training_set,\n",
    "                 control = rpart.control(\n",
    "                             cp = 3e-5, \n",
    "                             xval = 10))\n",
    "printcp(m0_rpart)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9d8bfd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plotcp(m0_rpart)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a950bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "head(m0_rpart$cptable, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e77ea71b",
   "metadata": {},
   "source": [
    "Let us see the optimal tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d3926ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "cp_star = m0_rpart$cptable[which.min(m0_rpart$cptable[, 4]), 1]\n",
    "print(cp_star)\n",
    "\n",
    "rpart.plot(prune(m0_rpart, cp = cp_star), type = 5, extra = 101, under = FALSE, fallen.leaves = FALSE,\n",
    "    digits = 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e717ab25",
   "metadata": {},
   "source": [
    "Using the 1-SE error, we need to compute the greatest cp parameters that enters the 1-SE window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e596b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "SE = m0_rpart$cptable[which.min(m0_rpart$cptable[, 4]), 5]\n",
    "min_error = m0_rpart$cptable[which.min(m0_rpart$cptable[, 4]), 4]\n",
    "boundary = min_error + SE\n",
    "boundary\n",
    "\n",
    "# We need to find the first cp such that xerror < boundary.\n",
    "row_optimal = which(m0_rpart$cptable[, 4] < boundary)[1]\n",
    "cp_star = m0_rpart$cptable[row_optimal, 1]\n",
    "print(cp_star)\n",
    "\n",
    "rpart.plot(prune(m0_rpart, cp = cp_star), type = 5, extra = 101, under = FALSE, fallen.leaves = FALSE,\n",
    "    digits = 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56876c86",
   "metadata": {},
   "source": [
    "## Using all covariates\n",
    "\n",
    "Let us now include all the covariates from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9f20d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "m1_rpart = rpart(cbind(Exposure, ClaimNb) ~ Power + CarAge + DriverAge + Brand +\n",
    "    Gas + Region + Density, data = training_set, method = \"poisson\", control = rpart.control(cp = 0,\n",
    "    xval = 10, minbucket = 1000))\n",
    "printcp(m1_rpart)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41310f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plotcp(x = m1_rpart, minline = TRUE, col = \"red\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebfcad1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute boundary for 1SE-rule\n",
    "SE = m1_rpart$cptable[which.min(m1_rpart$cptable[, 4]), 5]\n",
    "min_error = m1_rpart$cptable[which.min(m1_rpart$cptable[, 4]), 4]\n",
    "boundary = min_error + SE\n",
    "\n",
    "# Plot cross-validation error and 1SE_rule boundary.\n",
    "ggplot() + geom_line(aes(x = m1_rpart$cptable[, 1], y = m1_rpart$cptable[, 4]))+\n",
    "geom_point(aes(x = m1_rpart$cptable[, 1], y = m1_rpart$cptable[, 4]))+\n",
    "geom_hline(aes(yintercept = boundary), color=\"red\")+\n",
    "scale_x_continuous(name = \"CP\") + scale_y_continuous(name = \"xerror\") + theme_bw()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8472f603",
   "metadata": {},
   "source": [
    "If we take the value of cp that minimizes the error, we find"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a4baf12",
   "metadata": {},
   "outputs": [],
   "source": [
    "cp_star = m1_rpart$cptable[which.min(m1_rpart$cptable[, 4]), 1]\n",
    "cp_star"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a283ed80",
   "metadata": {},
   "source": [
    "Let us plot the optimal tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "300fc649",
   "metadata": {},
   "outputs": [],
   "source": [
    "m2_rpart = prune(m1_rpart, cp = cp_star)\n",
    "rpart.plot(m2_rpart, type = 5, extra = 101, under = FALSE, fallen.leaves = TRUE,\n",
    "    digits = 3, cex = 0.60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba2232a0",
   "metadata": {},
   "source": [
    "There is a possibility to extract a variable importance metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a9dd69",
   "metadata": {},
   "outputs": [],
   "source": [
    "plotdata = data.frame(m2_rpart$variable.importance)\n",
    "names(plotdata) = 'importance'\n",
    "plotdata$var = rownames(plotdata)\n",
    "\n",
    "ggplot(plotdata,aes(x =reorder(var,importance), y=importance)) + \n",
    "geom_bar(stat='identity')+coord_flip()+\n",
    "scale_x_discrete(name=\"Variable\") + \n",
    "theme_bw()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5038f69",
   "metadata": {},
   "source": [
    "Finally, let us compute the deviance on the testing_set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44bbbea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "deviance_poisson = function(x_obs, x_pred) {\n",
    "    2 * (sum(dpois(x = x_obs, lambda = x_obs, log = TRUE)) - sum(dpois(x = x_obs,\n",
    "        lambda = x_pred, log = TRUE)))\n",
    "}\n",
    "\n",
    "deviance_poisson(x_obs = testing_set$ClaimNb, x_pred = predict(m2_rpart, testing_set) *\n",
    "    testing_set$Exposure)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6acc2bb",
   "metadata": {},
   "source": [
    "If we compute the deviance on the full tree (not the pruned tree), we obtain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f0c673e",
   "metadata": {},
   "outputs": [],
   "source": [
    "deviance_poisson(x_obs = testing_set$ClaimNb, x_pred = predict(m1_rpart, testing_set) *\n",
    "    testing_set$Exposure)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b5ba54b",
   "metadata": {},
   "source": [
    "## Bagging of trees"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf444985",
   "metadata": {},
   "source": [
    "Let us create the bootstrap samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef4a101e",
   "metadata": {},
   "outputs": [],
   "source": [
    "set.seed(85)\n",
    "bootstrap_samples = createResample(training_set$ClaimNb, times = 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b36049f7",
   "metadata": {},
   "source": [
    "For each sample, we estimate a CART with the optimal complexity parameter found previously. Each tree, gives us an estimation of the claim frequency, which we average."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0188bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "bagg_cart = lapply(bootstrap_samples, function(X) {\n",
    "    rpart(cbind(Exposure, ClaimNb) ~ Power + CarAge + DriverAge + Brand + Gas + Region + Density, \n",
    "          data = training_set[X, ], \n",
    "          method = \"poisson\", \n",
    "          control = rpart.control(cp = cp_star,\n",
    "                                  xval = 0))\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19140aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = lapply(bagg_cart, function(X) {\n",
    "    predict(X, testing_set) * testing_set$Exposure\n",
    "})\n",
    "\n",
    "pred = do.call(cbind, pred)\n",
    "pred = apply(pred, 1, mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62712f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "deviance_poisson(x_obs = testing_set$ClaimNb, x_pred = pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.1.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
